# This file contains logic for running evaluations on TractoAI: <https://tracto.ai/>.

import yt.wrapper as yt
import yt.type_info as ti
from typing import Iterable
import datetime
import dataclasses
import sys
from yt import yson

from pathlib import Path
from swebench.harness.docker_build import setup_logger
from swebench.harness.reporting import make_run_report
from swebench.harness.utils import EvaluationError
from typing import cast, Annotated


from swebench.harness.constants import (
    APPLY_PATCH_FAIL,
    APPLY_PATCH_PASS,
    RUN_EVALUATION_LOG_DIR,
    SWEbenchInstance,
)
from swebench.harness.grading import get_eval_report
from swebench.harness.test_spec.test_spec import make_test_spec, TestSpec


yt.config["pickling"]["ignore_system_modules"] = True
yt.config["pickling"]["dynamic_libraries"]["enable_auto_collection"] = False


@yt.yt_dataclass
class TestInput:
    test_spec: Annotated[
        bytes,
        yt.schema.types.Annotation(
            ti_type=ti.Yson,
            to_yt_type=lambda x: yson.dumps(dataclasses.asdict(x)),
            from_yt_type=lambda x: TestSpec(**yson.loads(x)),
        )
    ] | None
    prediction: Annotated[
        bytes,
        yt.schema.types.Annotation(
            ti_type=ti.Yson,
            to_yt_type=lambda x: yson.dumps(x),
            from_yt_type=lambda x: yson.loads(x)
        )
    ] | None
    run_id: str
    timeout: int


@yt.yt_dataclass
class TestOutput:
    instance_id: str
    test_output: str
    report_json_str: str
    run_instance_log: str
    patch_diff: str
    # log_dir: Path
    errored: bool


def get_log_dir(pred: dict, run_id: str, instance_id: str) -> Path:
    model_name_or_path = cast(
        str, pred.get("model_name_or_path", "None").replace("/", "__")
    )
    return RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id

class RunInstanceTracto(yt.TypedJob):
    def __call__(self, test_input: TestInput) -> Iterable[TestOutput]:
        print(f"{test_input=}", file=sys.stderr)
        yield TestOutput(
            instance_id=test_input.test_spec.instance_id,
            test_output="test_output_dummy",
            report_json_str="dummy_report_json_str",
            run_instance_log="dummy_run_instance_log",
            patch_diff="dummy_patch_diff",
            errored=False
        )


def run_instances_tracto(
    predictions: dict[str, dict],
    instances: list[SWEbenchInstance],
    full_dataset: list[SWEbenchInstance],
    run_id: str,
    timeout: int,
):
    """
    Run all instances for the given predictions on Modal.

    Args:
        predictions (dict): Predictions dict generated by the model
        instances (list): List of instances
        run_id (str): Run ID
        timeout (int): Timeout for running tests
    """
    test_specs = [make_test_spec(instance) for instance in instances]

    run_test_specs: list[TestSpec] = []

    # steps:
    # 1. upload source table
    # 2. run map, save results to output table
    # 3. parse output table

    # Check for instances that have already been run
    for test_spec in test_specs:
        log_dir = get_log_dir(
            predictions[test_spec.instance_id], run_id, test_spec.instance_id
        )
        if log_dir.exists():
            continue
        run_test_specs.append(test_spec)

    if run_test_specs:
        run_dir = (
            "//home/llm/sbkarasik/tracto-swe-bench/"
            f"{run_id}-{datetime.datetime.now().isoformat()}"
        )
        print(f"{run_dir=}")

        yt.create("map_node", run_dir, recursive=True)

        input_table_path = f"{run_dir}/input"
        output_table_path = f"{run_dir}/output"
        print(f"{input_table_path=}")

        source_table_rows = [
            TestInput(
                test_spec=test_spec,
                prediction=predictions[test_spec.instance_id],
                run_id=run_id,
                timeout=timeout,
            )
            for test_spec in run_test_specs
        ]
        yt.write_table_structured(
            table=input_table_path,
            row_type=TestInput,
            input_stream=source_table_rows,
        )

        yt.run_map(
            RunInstanceTracto(),
            input_table_path,
            output_table_path,
            spec={
                "mapper": {
                    "docker_image": "cr.turing.yt.nebius.yt/home/llm/sbkarasik/registry/swebench-fork:dev",
                },
                "max_failed_job_count": 1,
            }
        )

    #     for result in results:
    #         result = cast(TestOutput, result)

    #         # Save logs locally
    #         log_dir = result.log_dir
    #         log_dir.mkdir(parents=True, exist_ok=True)
    #         with open(log_dir / "run_instance.log", "w") as f:
    #             f.write(result.run_instance_log)
    #         with open(log_dir / "test_output.txt", "w") as f:
    #             f.write(result.test_output)
    #         with open(log_dir / "patch.diff", "w") as f:
    #             f.write(result.patch_diff)
    #         with open(log_dir / "report.json", "w") as f:
    #             try:
    #                 report_json = json.loads(result.report_json_str)
    #                 json.dump(report_json, f, indent=4)
    #             except Exception:
    #                 # This happens if the test fails with any exception
    #                 print(f"{result.instance_id}: no report.json")

    # make_run_report(predictions, full_dataset, run_id)


def main(
    dataset_name: str,
    split: str,
    instance_ids: list,
    predictions_path: str,
    max_workers: int,
    force_rebuild: bool,
    cache_level: str,
    clean: bool,
    open_file_limit: int,
    run_id: str,
    timeout: int,
    namespace: str | None,
    rewrite_reports: bool,
    modal: bool,
    # tracto: bool,
    instance_image_tag: str = "latest",
    report_dir: str = ".",
):
    """
    Run evaluation harness for the given dataset and predictions.
    """
    from swebench.harness.constants import (
        KEY_INSTANCE_ID,
    )

    from swebench.harness.utils import (
        load_swebench_dataset,
        get_predictions_from_file,
    )

    from swebench.harness.run_evaluation import get_dataset_from_preds

    namespace = None if namespace == "" else namespace

    # load predictions as map of instance_id to prediction
    predictions = get_predictions_from_file(predictions_path, dataset_name, split)
    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}

    # get dataset from predictions
    dataset = get_dataset_from_preds(
        dataset_name, split, instance_ids, predictions, run_id, rewrite_reports
    )
    full_dataset = load_swebench_dataset(dataset_name, split, instance_ids)

    run_instances_tracto(predictions, dataset, full_dataset, run_id, timeout)


if __name__ == "__main__":
    import argparse

    from swebench.harness.utils import str2bool

    parser = argparse.ArgumentParser(
        description="Run evaluation harness for the given dataset and predictions.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Common args
    parser.add_argument(
        "--dataset_name",
        default="SWE-bench/SWE-bench_Lite",
        type=str,
        help="Name of dataset or path to JSON file.",
    )
    parser.add_argument(
        "--split", type=str, default="test", help="Split of the dataset"
    )
    parser.add_argument(
        "--instance_ids",
        nargs="+",
        type=str,
        help="Instance IDs to run (space separated)",
    )
    parser.add_argument(
        "--predictions_path",
        type=str,
        help="Path to predictions file - if 'gold', uses gold predictions",
        required=True,
    )

    # Local execution args
    parser.add_argument(
        "--max_workers",
        type=int,
        default=4,
        help="Maximum number of workers (should be <= 75%% of CPU cores)",
    )
    parser.add_argument(
        "--open_file_limit", type=int, default=4096, help="Open file limit"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=1_800,
        help="Timeout (in seconds) for running tests for each instance",
    )
    parser.add_argument(
        "--force_rebuild",
        type=str2bool,
        default=False,
        help="Force rebuild of all images",
    )
    parser.add_argument(
        "--cache_level",
        type=str,
        choices=["none", "base", "env", "instance"],
        help="Cache level - remove images above this level",
        default="env",
    )
    # if clean is true then we remove all images that are above the cache level
    # if clean is false, we only remove images above the cache level if they don't already exist
    parser.add_argument(
        "--clean", type=str2bool, default=False, help="Clean images above cache level"
    )
    parser.add_argument(
        "--run_id", type=str, required=True, help="Run ID - identifies the run"
    )
    parser.add_argument(
        "--namespace", type=str, default="swebench", help="Namespace for images"
    )
    parser.add_argument(
        "--instance_image_tag", type=str, default="latest", help="Instance image tag"
    )
    parser.add_argument(
        "--rewrite_reports",
        type=str2bool,
        default=False,
        help="Doesn't run new instances, only writes reports for instances with existing test outputs",
    )
    parser.add_argument(
        "--report_dir", type=str, default=".", help="Directory to write reports to"
    )

    # Modal execution args
    parser.add_argument("--modal", type=str2bool, default=False, help="Run on Modal")

    # Traco execution args
    # parser.add_argument("--tracto", type=str2bool, default=False, help="Run on Tracto")

    args = parser.parse_args()
    main(**vars(args))
